{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churner Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset/data?select=Bank+Customer+Churn+Prediction.csv\">Bank Customer Churn Dataset</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<a href=\"https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset/data?select=Bank+Customer+Churn+Prediction.csv\">Bank Customer Churn Dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import train_test_split \n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Bank Customer Churn Prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France' 'Spain' 'Germany']\n",
      "['Female' 'Male']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"country\"].unique())\n",
    "print(df[\"gender\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>products_number</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>churn</th>\n",
       "      <th>country_Germany</th>\n",
       "      <th>country_Spain</th>\n",
       "      <th>gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15634602</td>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15647311</td>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15619304</td>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15701354</td>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15737888</td>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  credit_score  age  tenure    balance  products_number  \\\n",
       "0     15634602           619   42       2       0.00                1   \n",
       "1     15647311           608   41       1   83807.86                1   \n",
       "2     15619304           502   42       8  159660.80                3   \n",
       "3     15701354           699   39       1       0.00                2   \n",
       "4     15737888           850   43       2  125510.82                1   \n",
       "\n",
       "   credit_card  active_member  estimated_salary  churn  country_Germany  \\\n",
       "0            1              1         101348.88      1            False   \n",
       "1            0              1         112542.58      0            False   \n",
       "2            1              0         113931.57      1            False   \n",
       "3            0              0          93826.63      0            False   \n",
       "4            1              1          79084.10      0            False   \n",
       "\n",
       "   country_Spain  gender_Male  \n",
       "0          False        False  \n",
       "1           True        False  \n",
       "2          False        False  \n",
       "3          False        False  \n",
       "4           True        False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['country', 'gender'], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churn\n",
       "0    7963\n",
       "1    2037\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['churn'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500, 12), (2500, 12))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop([\"churn\"], axis=1) \n",
    "y = df['churn'] \n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) \n",
    "X_train.shape, X_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90      2252\n",
      "           1       0.34      0.68      0.45       248\n",
      "\n",
      "    accuracy                           0.84      2500\n",
      "   macro avg       0.65      0.77      0.68      2500\n",
      "weighted avg       0.90      0.84      0.86      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators = 2) \n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# predict the model \n",
    "y_pred = model.predict(X_test) \n",
    "\n",
    "# performance evaluation metrics \n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmonotonic_cst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mForestClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    A random forest classifier.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    A random forest is a meta estimator that fits a number of decision tree\u001b[0m\n",
      "\u001b[0;34m    classifiers on various sub-samples of the dataset and uses averaging to\u001b[0m\n",
      "\u001b[0;34m    improve the predictive accuracy and control over-fitting.\u001b[0m\n",
      "\u001b[0;34m    Trees in the forest use the best split strategy, i.e. equivalent to passing\u001b[0m\n",
      "\u001b[0;34m    `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\u001b[0m\n",
      "\u001b[0;34m    The sub-sample size is controlled with the `max_samples` parameter if\u001b[0m\n",
      "\u001b[0;34m    `bootstrap=True` (default), otherwise the whole dataset is used to build\u001b[0m\n",
      "\u001b[0;34m    each tree.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For a comparison between tree-based ensemble models see the example\u001b[0m\n",
      "\u001b[0;34m    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Read more in the :ref:`User Guide <forest>`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    n_estimators : int, default=100\u001b[0m\n",
      "\u001b[0;34m        The number of trees in the forest.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 0.22\u001b[0m\n",
      "\u001b[0;34m           The default value of ``n_estimators`` changed from 10 to 100\u001b[0m\n",
      "\u001b[0;34m           in 0.22.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\u001b[0m\n",
      "\u001b[0;34m        The function to measure the quality of a split. Supported criteria are\u001b[0m\n",
      "\u001b[0;34m        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\u001b[0m\n",
      "\u001b[0;34m        Shannon information gain, see :ref:`tree_mathematical_formulation`.\u001b[0m\n",
      "\u001b[0;34m        Note: This parameter is tree-specific.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    max_depth : int, default=None\u001b[0m\n",
      "\u001b[0;34m        The maximum depth of the tree. If None, then nodes are expanded until\u001b[0m\n",
      "\u001b[0;34m        all leaves are pure or until all leaves contain less than\u001b[0m\n",
      "\u001b[0;34m        min_samples_split samples.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    min_samples_split : int or float, default=2\u001b[0m\n",
      "\u001b[0;34m        The minimum number of samples required to split an internal node:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - If int, then consider `min_samples_split` as the minimum number.\u001b[0m\n",
      "\u001b[0;34m        - If float, then `min_samples_split` is a fraction and\u001b[0m\n",
      "\u001b[0;34m          `ceil(min_samples_split * n_samples)` are the minimum\u001b[0m\n",
      "\u001b[0;34m          number of samples for each split.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 0.18\u001b[0m\n",
      "\u001b[0;34m           Added float values for fractions.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    min_samples_leaf : int or float, default=1\u001b[0m\n",
      "\u001b[0;34m        The minimum number of samples required to be at a leaf node.\u001b[0m\n",
      "\u001b[0;34m        A split point at any depth will only be considered if it leaves at\u001b[0m\n",
      "\u001b[0;34m        least ``min_samples_leaf`` training samples in each of the left and\u001b[0m\n",
      "\u001b[0;34m        right branches.  This may have the effect of smoothing the model,\u001b[0m\n",
      "\u001b[0;34m        especially in regression.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - If int, then consider `min_samples_leaf` as the minimum number.\u001b[0m\n",
      "\u001b[0;34m        - If float, then `min_samples_leaf` is a fraction and\u001b[0m\n",
      "\u001b[0;34m          `ceil(min_samples_leaf * n_samples)` are the minimum\u001b[0m\n",
      "\u001b[0;34m          number of samples for each node.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 0.18\u001b[0m\n",
      "\u001b[0;34m           Added float values for fractions.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    min_weight_fraction_leaf : float, default=0.0\u001b[0m\n",
      "\u001b[0;34m        The minimum weighted fraction of the sum total of weights (of all\u001b[0m\n",
      "\u001b[0;34m        the input samples) required to be at a leaf node. Samples have\u001b[0m\n",
      "\u001b[0;34m        equal weight when sample_weight is not provided.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\u001b[0m\n",
      "\u001b[0;34m        The number of features to consider when looking for the best split:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - If int, then consider `max_features` features at each split.\u001b[0m\n",
      "\u001b[0;34m        - If float, then `max_features` is a fraction and\u001b[0m\n",
      "\u001b[0;34m          `max(1, int(max_features * n_features_in_))` features are considered at each\u001b[0m\n",
      "\u001b[0;34m          split.\u001b[0m\n",
      "\u001b[0;34m        - If \"sqrt\", then `max_features=sqrt(n_features)`.\u001b[0m\n",
      "\u001b[0;34m        - If \"log2\", then `max_features=log2(n_features)`.\u001b[0m\n",
      "\u001b[0;34m        - If None, then `max_features=n_features`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 1.1\u001b[0m\n",
      "\u001b[0;34m            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Note: the search for a split does not stop until at least one\u001b[0m\n",
      "\u001b[0;34m        valid partition of the node samples is found, even if it requires to\u001b[0m\n",
      "\u001b[0;34m        effectively inspect more than ``max_features`` features.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    max_leaf_nodes : int, default=None\u001b[0m\n",
      "\u001b[0;34m        Grow trees with ``max_leaf_nodes`` in best-first fashion.\u001b[0m\n",
      "\u001b[0;34m        Best nodes are defined as relative reduction in impurity.\u001b[0m\n",
      "\u001b[0;34m        If None then unlimited number of leaf nodes.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    min_impurity_decrease : float, default=0.0\u001b[0m\n",
      "\u001b[0;34m        A node will be split if this split induces a decrease of the impurity\u001b[0m\n",
      "\u001b[0;34m        greater than or equal to this value.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        The weighted impurity decrease equation is the following::\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            N_t / N * (impurity - N_t_R / N_t * right_impurity\u001b[0m\n",
      "\u001b[0;34m                                - N_t_L / N_t * left_impurity)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        where ``N`` is the total number of samples, ``N_t`` is the number of\u001b[0m\n",
      "\u001b[0;34m        samples at the current node, ``N_t_L`` is the number of samples in the\u001b[0m\n",
      "\u001b[0;34m        left child, and ``N_t_R`` is the number of samples in the right child.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\u001b[0m\n",
      "\u001b[0;34m        if ``sample_weight`` is passed.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.19\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    bootstrap : bool, default=True\u001b[0m\n",
      "\u001b[0;34m        Whether bootstrap samples are used when building trees. If False, the\u001b[0m\n",
      "\u001b[0;34m        whole dataset is used to build each tree.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    oob_score : bool or callable, default=False\u001b[0m\n",
      "\u001b[0;34m        Whether to use out-of-bag samples to estimate the generalization score.\u001b[0m\n",
      "\u001b[0;34m        By default, :func:`~sklearn.metrics.accuracy_score` is used.\u001b[0m\n",
      "\u001b[0;34m        Provide a callable with signature `metric(y_true, y_pred)` to use a\u001b[0m\n",
      "\u001b[0;34m        custom metric. Only available if `bootstrap=True`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    n_jobs : int, default=None\u001b[0m\n",
      "\u001b[0;34m        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\u001b[0m\n",
      "\u001b[0;34m        :meth:`decision_path` and :meth:`apply` are all parallelized over the\u001b[0m\n",
      "\u001b[0;34m        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\u001b[0m\n",
      "\u001b[0;34m        context. ``-1`` means using all processors. See :term:`Glossary\u001b[0m\n",
      "\u001b[0;34m        <n_jobs>` for more details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    random_state : int, RandomState instance or None, default=None\u001b[0m\n",
      "\u001b[0;34m        Controls both the randomness of the bootstrapping of the samples used\u001b[0m\n",
      "\u001b[0;34m        when building trees (if ``bootstrap=True``) and the sampling of the\u001b[0m\n",
      "\u001b[0;34m        features to consider when looking for the best split at each node\u001b[0m\n",
      "\u001b[0;34m        (if ``max_features < n_features``).\u001b[0m\n",
      "\u001b[0;34m        See :term:`Glossary <random_state>` for details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    verbose : int, default=0\u001b[0m\n",
      "\u001b[0;34m        Controls the verbosity when fitting and predicting.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    warm_start : bool, default=False\u001b[0m\n",
      "\u001b[0;34m        When set to ``True``, reuse the solution of the previous call to fit\u001b[0m\n",
      "\u001b[0;34m        and add more estimators to the ensemble, otherwise, just fit a whole\u001b[0m\n",
      "\u001b[0;34m        new forest. See :term:`Glossary <warm_start>` and\u001b[0m\n",
      "\u001b[0;34m        :ref:`gradient_boosting_warm_start` for details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\u001b[0m\n",
      "\u001b[0;34m            default=None\u001b[0m\n",
      "\u001b[0;34m        Weights associated with classes in the form ``{class_label: weight}``.\u001b[0m\n",
      "\u001b[0;34m        If not given, all classes are supposed to have weight one. For\u001b[0m\n",
      "\u001b[0;34m        multi-output problems, a list of dicts can be provided in the same\u001b[0m\n",
      "\u001b[0;34m        order as the columns of y.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Note that for multioutput (including multilabel) weights should be\u001b[0m\n",
      "\u001b[0;34m        defined for each class of every column in its own dict. For example,\u001b[0m\n",
      "\u001b[0;34m        for four-class multilabel classification weights should be\u001b[0m\n",
      "\u001b[0;34m        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\u001b[0m\n",
      "\u001b[0;34m        [{1:1}, {2:5}, {3:1}, {4:1}].\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        The \"balanced\" mode uses the values of y to automatically adjust\u001b[0m\n",
      "\u001b[0;34m        weights inversely proportional to class frequencies in the input data\u001b[0m\n",
      "\u001b[0;34m        as ``n_samples / (n_classes * np.bincount(y))``\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        The \"balanced_subsample\" mode is the same as \"balanced\" except that\u001b[0m\n",
      "\u001b[0;34m        weights are computed based on the bootstrap sample for every tree\u001b[0m\n",
      "\u001b[0;34m        grown.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        For multi-output, the weights of each column of y will be multiplied.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Note that these weights will be multiplied with sample_weight (passed\u001b[0m\n",
      "\u001b[0;34m        through the fit method) if sample_weight is specified.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ccp_alpha : non-negative float, default=0.0\u001b[0m\n",
      "\u001b[0;34m        Complexity parameter used for Minimal Cost-Complexity Pruning. The\u001b[0m\n",
      "\u001b[0;34m        subtree with the largest cost complexity that is smaller than\u001b[0m\n",
      "\u001b[0;34m        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\u001b[0m\n",
      "\u001b[0;34m        :ref:`minimal_cost_complexity_pruning` for details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.22\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    max_samples : int or float, default=None\u001b[0m\n",
      "\u001b[0;34m        If bootstrap is True, the number of samples to draw from X\u001b[0m\n",
      "\u001b[0;34m        to train each base estimator.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - If None (default), then draw `X.shape[0]` samples.\u001b[0m\n",
      "\u001b[0;34m        - If int, then draw `max_samples` samples.\u001b[0m\n",
      "\u001b[0;34m        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\u001b[0m\n",
      "\u001b[0;34m          `max_samples` should be in the interval `(0.0, 1.0]`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.22\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    monotonic_cst : array-like of int of shape (n_features), default=None\u001b[0m\n",
      "\u001b[0;34m        Indicates the monotonicity constraint to enforce on each feature.\u001b[0m\n",
      "\u001b[0;34m          - 1: monotonic increase\u001b[0m\n",
      "\u001b[0;34m          - 0: no constraint\u001b[0m\n",
      "\u001b[0;34m          - -1: monotonic decrease\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        If monotonic_cst is None, no constraints are applied.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Monotonicity constraints are not supported for:\u001b[0m\n",
      "\u001b[0;34m          - multiclass classifications (i.e. when `n_classes > 2`),\u001b[0m\n",
      "\u001b[0;34m          - multioutput classifications (i.e. when `n_outputs_ > 1`),\u001b[0m\n",
      "\u001b[0;34m          - classifications trained on data with missing values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        The constraints hold over the probability of the positive class.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 1.4\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Attributes\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\u001b[0m\n",
      "\u001b[0;34m        The child estimator template used to create the collection of fitted\u001b[0m\n",
      "\u001b[0;34m        sub-estimators.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 1.2\u001b[0m\n",
      "\u001b[0;34m           `base_estimator_` was renamed to `estimator_`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    estimators_ : list of DecisionTreeClassifier\u001b[0m\n",
      "\u001b[0;34m        The collection of fitted sub-estimators.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    classes_ : ndarray of shape (n_classes,) or a list of such arrays\u001b[0m\n",
      "\u001b[0;34m        The classes labels (single output problem), or a list of arrays of\u001b[0m\n",
      "\u001b[0;34m        class labels (multi-output problem).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    n_classes_ : int or list\u001b[0m\n",
      "\u001b[0;34m        The number of classes (single output problem), or a list containing the\u001b[0m\n",
      "\u001b[0;34m        number of classes for each output (multi-output problem).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    n_features_in_ : int\u001b[0m\n",
      "\u001b[0;34m        Number of features seen during :term:`fit`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.24\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[0m\n",
      "\u001b[0;34m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[0m\n",
      "\u001b[0;34m        has feature names that are all strings.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 1.0\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    n_outputs_ : int\u001b[0m\n",
      "\u001b[0;34m        The number of outputs when ``fit`` is performed.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    feature_importances_ : ndarray of shape (n_features,)\u001b[0m\n",
      "\u001b[0;34m        The impurity-based feature importances.\u001b[0m\n",
      "\u001b[0;34m        The higher, the more important the feature.\u001b[0m\n",
      "\u001b[0;34m        The importance of a feature is computed as the (normalized)\u001b[0m\n",
      "\u001b[0;34m        total reduction of the criterion brought by that feature.  It is also\u001b[0m\n",
      "\u001b[0;34m        known as the Gini importance.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Warning: impurity-based feature importances can be misleading for\u001b[0m\n",
      "\u001b[0;34m        high cardinality features (many unique values). See\u001b[0m\n",
      "\u001b[0;34m        :func:`sklearn.inspection.permutation_importance` as an alternative.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    oob_score_ : float\u001b[0m\n",
      "\u001b[0;34m        Score of the training dataset obtained using an out-of-bag estimate.\u001b[0m\n",
      "\u001b[0;34m        This attribute exists only when ``oob_score`` is True.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\u001b[0m\n",
      "\u001b[0;34m            (n_samples, n_classes, n_outputs)\u001b[0m\n",
      "\u001b[0;34m        Decision function computed with out-of-bag estimate on the training\u001b[0m\n",
      "\u001b[0;34m        set. If n_estimators is small it might be possible that a data point\u001b[0m\n",
      "\u001b[0;34m        was never left out during the bootstrap. In this case,\u001b[0m\n",
      "\u001b[0;34m        `oob_decision_function_` might contain NaN. This attribute exists\u001b[0m\n",
      "\u001b[0;34m        only when ``oob_score`` is True.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    estimators_samples_ : list of arrays\u001b[0m\n",
      "\u001b[0;34m        The subset of drawn samples (i.e., the in-bag samples) for each base\u001b[0m\n",
      "\u001b[0;34m        estimator. Each subset is defined by an array of the indices selected.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 1.4\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    See Also\u001b[0m\n",
      "\u001b[0;34m    --------\u001b[0m\n",
      "\u001b[0;34m    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\u001b[0m\n",
      "\u001b[0;34m    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\u001b[0m\n",
      "\u001b[0;34m        tree classifiers.\u001b[0m\n",
      "\u001b[0;34m    sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\u001b[0m\n",
      "\u001b[0;34m        Boosting Classification Tree, very fast for big datasets (n_samples >=\u001b[0m\n",
      "\u001b[0;34m        10_000).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Notes\u001b[0m\n",
      "\u001b[0;34m    -----\u001b[0m\n",
      "\u001b[0;34m    The default values for the parameters controlling the size of the trees\u001b[0m\n",
      "\u001b[0;34m    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\u001b[0m\n",
      "\u001b[0;34m    unpruned trees which can potentially be very large on some data sets. To\u001b[0m\n",
      "\u001b[0;34m    reduce memory consumption, the complexity and size of the trees should be\u001b[0m\n",
      "\u001b[0;34m    controlled by setting those parameter values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The features are always randomly permuted at each split. Therefore,\u001b[0m\n",
      "\u001b[0;34m    the best found split may vary, even with the same training data,\u001b[0m\n",
      "\u001b[0;34m    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\u001b[0m\n",
      "\u001b[0;34m    of the criterion is identical for several splits enumerated during the\u001b[0m\n",
      "\u001b[0;34m    search of the best split. To obtain a deterministic behaviour during\u001b[0m\n",
      "\u001b[0;34m    fitting, ``random_state`` has to be fixed.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    References\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples\u001b[0m\n",
      "\u001b[0;34m    --------\u001b[0m\n",
      "\u001b[0;34m    >>> from sklearn.ensemble import RandomForestClassifier\u001b[0m\n",
      "\u001b[0;34m    >>> from sklearn.datasets import make_classification\u001b[0m\n",
      "\u001b[0;34m    >>> X, y = make_classification(n_samples=1000, n_features=4,\u001b[0m\n",
      "\u001b[0;34m    ...                            n_informative=2, n_redundant=0,\u001b[0m\n",
      "\u001b[0;34m    ...                            random_state=0, shuffle=False)\u001b[0m\n",
      "\u001b[0;34m    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\u001b[0m\n",
      "\u001b[0;34m    >>> clf.fit(X, y)\u001b[0m\n",
      "\u001b[0;34m    RandomForestClassifier(...)\u001b[0m\n",
      "\u001b[0;34m    >>> print(clf.predict([[0, 0, 0, 0]]))\u001b[0m\n",
      "\u001b[0;34m    [1]\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mForestClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"class_weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mStrOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"balanced_subsample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"splitter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sqrt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmonotonic_cst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mestimator_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"criterion\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_depth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_samples_split\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_samples_leaf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_weight_fraction_leaf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_leaf_nodes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_impurity_decrease\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"random_state\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"ccp_alpha\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"monotonic_cst\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_impurity_decrease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic_cst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonotonic_cst\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mccp_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccp_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_forest.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "RandomForestClassifier??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {  \n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2', None], \n",
    "    'max_depth': [5, 10], \n",
    "    'max_leaf_nodes': [5, 10],\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Grid Search technique, we systematically explore every possible combination from a predefined list of hyperparameter values. This process resembles a grid, where values are arranged in a matrix. Each unique parameter set is carefully assessed, and the model's accuracy is recorded. After evaluating all combinations, the model with the parameter set that yields the highest accuracy is identified as the optimal choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'zack_is_the_best' for estimator RandomForestClassifier(max_depth=5, max_leaf_nodes=5, n_estimators=2). Valid parameters are: ['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'monotonic_cst', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid) \n\u001b[0;32m----> 2\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train) \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    918\u001b[0m         clone(base_estimator),\n\u001b[1;32m    919\u001b[0m         X,\n\u001b[1;32m    920\u001b[0m         y,\n\u001b[1;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:883\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    876\u001b[0m score_params_test \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mscore_params, indices\u001b[38;5;241m=\u001b[39mtest)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;66;03m# here we clone the parameters, since sometimes the parameters\u001b[39;00m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# themselves might be estimators, e.g. when we search over different\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;66;03m# estimators in a pipeline.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# ref: https://github.com/scikit-learn/scikit-learn/pull/26786\u001b[39;00m\n\u001b[0;32m--> 883\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclone(parameters, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m    885\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    887\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:279\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_params:\n\u001b[1;32m    278\u001b[0m     local_valid_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names()\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for estimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid parameters are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_valid_params\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delim:\n\u001b[1;32m    285\u001b[0m     nested_params[key][sub_key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter 'zack_is_the_best' for estimator RandomForestClassifier(max_depth=5, max_leaf_nodes=5, n_estimators=2). Valid parameters are: ['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'monotonic_cst', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start']."
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(model, param_grid=param_grid) \n",
    "grid_search.fit(X_train, y_train) \n",
    "print(grid_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90      2252\n",
      "           1       0.34      0.68      0.45       248\n",
      "\n",
      "    accuracy                           0.84      2500\n",
      "   macro avg       0.65      0.77      0.68      2500\n",
      "weighted avg       0.90      0.84      0.86      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_grid = grid_search.best_estimator_\n",
    "y_pred_grid = model.predict(X_test) \n",
    "print(classification_report(y_pred_grid, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search involves exploring various combinations of hyperparameters randomly to discover the optimal solution for a constructed model. This technique tests out random combinations within specified value ranges. To optimize through random search, the function is assessed at a certain number of randomly selected configurations within the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='entropy', max_depth=5, max_features=None,\n",
      "                       max_leaf_nodes=10, n_estimators=2)\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(model,param_grid) \n",
    "random_search.fit(X_train, y_train) \n",
    "print(random_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90      2254\n",
      "           1       0.34      0.69      0.45       246\n",
      "\n",
      "    accuracy                           0.84      2500\n",
      "   macro avg       0.65      0.77      0.68      2500\n",
      "weighted avg       0.90      0.84      0.86      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_random = random_search.best_estimator_\n",
    "y_pred_rand = model.predict(X_test) \n",
    "print(classification_report(y_pred_rand, y_test)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
