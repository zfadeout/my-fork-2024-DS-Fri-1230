{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall 2024 Data Science Track: Week 5 - Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Packages, Packages!\n",
    "\n",
    "Import *all* the things here! You need: `matplotlib`, `networkx`, `numpy`, and `pandas`â€•and also `ast.literal_eval` to correctly deserialize two columns in the `rules.tsv.xz` file.\n",
    "\n",
    "If you got more stuff you want to use, add them here too. ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donâ€™t worry about this. This is needed to interpret the Python code that is embedded in the data set. You only need it literally in the very next code cell and nowhere else. \n",
    "from ast import literal_eval\n",
    "\n",
    "# The rest is just the stuff from the lecture.\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "With the packages out of the way, now you will be working with the Instacart association rules data set, mined from the [Instacart Market Basket Analysis data set](https://www.kaggle.com/c/instacart-market-basket-analysis/data) on Kaggle. [The script](https://github.com/LiKenun/shopping-assistant/blob/main/api/preprocess_instacart_market_basket_analysis_data.py) that does it and the instructions to run it can be found in my [Shopping Assistant Project](https://github.com/LiKenun/shopping-assistant) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "This code has already been pre-written, simply because there are a few quirks which require converters to ensure the correct deserialization of some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_data_path = 'data/rules.tsv.xz'                       # You do not need to decompress this yourself. Pandas understands how to read compressed data.\n",
    "\n",
    "df_rules = pd.read_csv(rules_data_path,\n",
    "                       sep='\\t',\n",
    "                       quoting=3,                           # This disables interpretation of quotes by Pandas itself, because both single and double quotes will be resolved by literal_eval.\n",
    "                       converters={\n",
    "                           'consequent_item': literal_eval,\n",
    "                           'antecedent_items': literal_eval # This reads something like [\"Grandma's 8\\\" Chip Cookies\", '6\" Apple Pie'] into a list, so you will get a column where each individual cell is a list.\n",
    "                       },\n",
    "                       low_memory=True)                     # For Chris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just *how* many rules were just loadedâ€½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the list of column names and the number of rules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Compute the support, confidence, and lift of each rule.\n",
    "\n",
    "* The ruleâ€™s *support* tells you how frequently the set of items appears in the dataset. Itâ€™s important to prune infrequent sets from further consideration.\n",
    "    * The simple definition: $$P(A \\cap B)$$\n",
    "    * `= item_set_count / transaction_count`\n",
    "* The ruleâ€™s *confidence* tells you how often a the rule is true. Divide the support for the set of items by the support for just the antecedents. Rules which are not true very often are also pruned.\n",
    "    * The simple definition: $$\\frac{P(A \\cap B)}{P(A)}$$\n",
    "    * `= item_set_count / transaction_count / (antecedent_count / transaction_count)`\n",
    "    * `= item_set_count / antecedent_count`\n",
    "* The ruleâ€™s *lift* tells you how much more likely the consequent is, given the antecedents, compared to its baseline probability. Divide the support for the set of items by both the support of the antecedents and consequent. Equivalently, divide the confidence by the support of the consequent.\n",
    "    * The simple definition: $$\\frac{P(A \\cap B)}{P(A) \\cdot P(B)}$$\n",
    "    * `= item_set_count / transaction_count / (antecedent_count / transaction_count * (consequent_count / transaction_count))`\n",
    "    * `= item_set_count / antecedent_count / (consequent_count / transaction_count)`\n",
    "    * `= item_set_count * transaction_count / (antecedent_count * consequent_count)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns support, confidence, and lift to df_rules. And show the first 50 rules.\n",
    "\n",
    "df_rules = # Something goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yogurts have got some insane lift (*over 9,000*). Why do you think that might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Write your answer here.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the rule set if you have to to find out more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Visualization for Consequents with Single Antecedents\n",
    "\n",
    "Letâ€™s now visualize a small subset of 1,000,000+ rules. First, filter the rule set for the following to whittle it down to something more manageable:\n",
    "\n",
    "1. The rule must have exactly `1` antecedent item. (There should be 38,684 such rules.)\n",
    "2. The lift must be between `5` and `20`. (There should be 1,596 such rules, including the prior criterion.)\n",
    "3. Either the antecedent or consequent of the rule must contain `'Hummus'`, but not both. (This should get you down to 26 rules.)\n",
    "    * Convert the antecedents `list`-typed column to a `str`-typed column (`antecedent_item`) since there will only be a single antecedent in the subset.\n",
    "    * Replace any item containing `'Hummus'` to just `'Hummus'`. This will make the visualization more readable later.\n",
    "\n",
    "Hint: your code may run more efficiently if you re-order certain processing steps.\n",
    "\n",
    "Assign the subset to `df_rules_subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define df_rules_subset.\n",
    "\n",
    "df_rules_subset = # Something goes here. You can repeat this for as many steps as necessary. Technically, you can do it all in one shot. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a network `graph_rules_subset` from the association rules subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph_rules_subset, add the graphâ€™s edges, and plot it. You may need a large figure size, smaller node size, and smaller font size.\n",
    "\n",
    "graph_rules_subset = nx.MultiDiGraph()\n",
    "graph_rules_subset.add_edges_from(\n",
    "    # Something goes here.\n",
    ")\n",
    "\n",
    "# Then render the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell about people who buy hummus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Write your answer here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Prediction\n",
    "\n",
    "Given that the basket of items contains the following items, use the full set of association rules to predict the next 20 most likely items (consequents) that the person will add to the basket in descending order of lift:\n",
    "\n",
    "* `'Orange Bell Pepper'`\n",
    "* `'Organic Red Bell Pepper'`\n",
    "\n",
    "Hint: a single item in the basket may be a better predictor of some consequents than both items considered together. You must consider both or either, but not neither."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = {'Orange Bell Pepper', 'Organic Red Bell Pepper'}\n",
    "\n",
    "df_rules[# A few conditions go here which takes care of all the following cases:\n",
    "         # * Just Orange Bell Pepper\n",
    "         # * Just Organic Red Bell Pepper\n",
    "         # * Both Orange Bell Pepper and Organic Red Bell Pepper\n",
    "         # You can do it using just 2 conditions. :)\n",
    "         ] \\\n",
    "        .sort_values('lift', ascending=False) \\\n",
    "        .head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Other Interesting Findings\n",
    "\n",
    "Find and share something else interesting about these association rules. It can be a graph, table, or some other format that illustrates your point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
